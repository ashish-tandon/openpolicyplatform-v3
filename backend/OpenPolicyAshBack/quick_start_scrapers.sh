#!/bin/bash
# Quick Start Scraper Testing
# ===========================
# This script immediately starts testing scrapers in parallel
# without getting stuck on individual scrapers.
# NOW WITH OPTIMIZED PARALLEL EXECUTION - Dynamic worker scaling (10-20)!

set -e  # Exit on any error

echo "ðŸš€ QUICK START - OPTIMIZED PARALLEL SCRAPER TESTING"
echo "=================================================="
echo ""
echo "This will start testing ALL scrapers with OPTIMIZED parallel execution."
echo "Dynamic worker scaling (10-20) based on scraper size and system resources!"
echo "No more waiting for individual scrapers to finish!"
echo ""

# Check if we're in the right directory
if [ ! -f "scraper_testing_framework.py" ]; then
    echo "âŒ Error: scraper_testing_framework.py not found"
    echo "Please run this script from the backend/OpenPolicyAshBack directory"
    exit 1
fi

# Check if virtual environment exists
if [ ! -d "venv" ]; then
    echo "âš ï¸  Virtual environment not found. Running setup first..."
    ./setup_scraper_testing.sh
fi

# Activate virtual environment
echo "ðŸ”§ Activating virtual environment..."
source venv/bin/activate

# Check database connection
echo "ðŸ” Checking database connection..."
if [ -z "$DATABASE_URL" ]; then
    echo "âš ï¸  DATABASE_URL not set. Using default..."
    export DATABASE_URL="postgresql://user:pass@localhost/openpolicy"
fi

# Display system information
echo ""
echo "ðŸ’» SYSTEM INFORMATION:"
echo "======================"
echo "CPU Cores: $(nproc 2>/dev/null || sysctl -n hw.ncpu 2>/dev/null || echo "Unknown")"
echo "Memory: $(free -h 2>/dev/null | grep Mem | awk '{print $2}' || echo "Unknown")"
echo "Python: $(python3 --version)"
echo ""

# Start optimized parallel testing immediately
echo ""
echo "ðŸŽ¯ STARTING OPTIMIZED PARALLEL SCRAPER TESTING"
echo "=============================================="
echo "Features:"
echo "  âœ… Dynamic worker scaling (10-20 workers)"
echo "  âœ… Size-based optimization (Small/Medium/Large scrapers)"
echo "  âœ… System resource monitoring"
echo "  âœ… 5 sample records per scraper"
echo "  âœ… Size-appropriate timeouts (3-10 minutes)"
echo "  âœ… Automatic error handling"
echo "  âœ… Real-time progress updates"
echo "  âœ… Database insertion testing"
echo "  âœ… Performance metrics tracking"
echo ""

# Run the optimized parallel testing framework
echo "ðŸš€ Starting optimized scraper testing framework..."
python3 scraper_testing_framework.py

echo ""
echo "âœ… OPTIMIZED PARALLEL TESTING COMPLETED!"
echo ""
echo "ðŸ“Š Check the results:"
echo "  - Test report: scraper_test_report_*.json"
echo "  - Logs: scraper_testing.log"
echo "  - Database: Check for inserted sample data"
echo "  - Performance: CPU and memory usage tracked"
echo ""
echo "ðŸ”„ Next steps:"
echo "  1. Review test results and fix any issues"
echo "  2. Run full data collection: python3 scraper_testing_framework.py --max-records 100"
echo "  3. Start background monitoring: python3 scraper_monitoring_system.py"
echo "  4. Optimize further based on performance metrics"
echo ""
echo "ðŸ“š Documentation:"
echo "  - SCRAPER_DEVELOPMENT_PLAN.md"
echo "  - REQUIREMENTS_MANAGEMENT.md"
echo ""
echo "ðŸŽ‰ All scrapers tested with optimized parallel execution!"
echo "   Dynamic scaling based on scraper size and system resources!"
